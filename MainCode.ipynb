{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import csv\n",
    "import os\n",
    "import dedupe\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from future.builtins import next\n",
    "from unidecode import unidecode\n",
    "import recordlinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Deduplication Problem - Sample Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Exact duplicates And PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.drop_duplicates(keep='first')\n",
    "df1['day'],df1['month']=df1['dob'].str.split('/', 1).str\n",
    "df1['month'],df1['year']=df1['month'].str.split('/', 1).str\n",
    "df1=df1.drop(['dob'],axis=1)\n",
    "df1[['day','month','year']] = df1[['day','month','year']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 1\n",
    "df1['fullname']=df1['fn']+\" \"+df1['ln']\n",
    "df1=df1.sort_values(by=['day','month','year','gn'])\n",
    "df1=df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of Exact duplicates\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print('Total No. of Exact duplicates')\n",
    "print(df.count()[0]-df1.count()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ln</th>\n",
       "      <th>gn</th>\n",
       "      <th>fn</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>fullname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SMITH JR</td>\n",
       "      <td>F</td>\n",
       "      <td>WILLIAM</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>WILLIAM SMITH JR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROTHMEYER JR</td>\n",
       "      <td>F</td>\n",
       "      <td>WILLIAM</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>WILLIAM ROTHMEYER JR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASBY JR</td>\n",
       "      <td>F</td>\n",
       "      <td>WILLIAM</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>WILLIAM ASBY JR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SALTER JR</td>\n",
       "      <td>F</td>\n",
       "      <td>WILLIAM</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>WILLIAM SALTER JR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MICHAELSON JR</td>\n",
       "      <td>M</td>\n",
       "      <td>ROY</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>ROY MICHAELSON JR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ln gn       fn  day  month  year              fullname\n",
       "0       SMITH JR  F  WILLIAM    1      3    68      WILLIAM SMITH JR\n",
       "1   ROTHMEYER JR  F  WILLIAM    1      3    68  WILLIAM ROTHMEYER JR\n",
       "2        ASBY JR  F  WILLIAM    1      3    68       WILLIAM ASBY JR\n",
       "3      SALTER JR  F  WILLIAM    1      3    68     WILLIAM SALTER JR\n",
       "4  MICHAELSON JR  M      ROY    1      5    51     ROY MICHAELSON JR"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding nearby duplicates for a string using fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pros(df1):\n",
    "    FULL_MATCHING_THRESHOLD = 80\n",
    "    PARTIAL_MATCHING_THRESHOLD = 100\n",
    "    SORT_MATCHING_THRESHOLD = 100\n",
    "    TOKEN_MATCHING_THRESHOLD = 100\n",
    "    MAX_MATCHES=4\n",
    "    current_db_dataframe=df1\n",
    "    \n",
    "    def find_matches(matchThis):\n",
    "        rows = current_db_dataframe['fullname'].values.tolist();\n",
    "        rows.remove(matchThis)\n",
    "        matches= process.extractBests(matchThis,rows,scorer=fuzz.ratio,score_cutoff=FULL_MATCHING_THRESHOLD,limit=MAX_MATCHES)\n",
    "        if len(matches)==0:\n",
    "            matches= process.extractBests(matchThis,rows,scorer=fuzz.partial_ratio,score_cutoff=PARTIAL_MATCHING_THRESHOLD,limit=MAX_MATCHES);\n",
    "            if len(matches)==0:\n",
    "                matches= process.extractBests(matchThis,rows,scorer=fuzz.token_set_ratio,score_cutoff=TOKEN_MATCHING_THRESHOLD,limit=MAX_MATCHES);\n",
    "                if len(matches)==0:\n",
    "                    matches= process.extractBests(matchThis,rows,scorer=fuzz.token_sort_ratio,score_cutoff=SORT_MATCHING_THRESHOLD,limit=MAX_MATCHES);\n",
    "        return matches[0][0] if len(matches)>0 else None\n",
    "\n",
    "\n",
    "    fn_find_matches = lambda x: find_matches(x)\n",
    "    current_db_dataframe['Duplicate']=current_db_dataframe.applymap(fn_find_matches)\n",
    "    current_db_dataframe.to_csv(\"using-fuzzy.csv\", index=False) #saving results to external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros(df1[[\"fullname\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullname</th>\n",
       "      <th>Duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WILLIAM SMITH JR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WILLIAM ROTHMEYER JR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WILLIAM ASBY JR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WILLIAM SALTER JR</td>\n",
       "      <td>WILLIAM SHAFFER JR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROY MICHAELSON JR</td>\n",
       "      <td>ROY MICHAELSON JR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fullname           Duplicate\n",
       "0      WILLIAM SMITH JR                 NaN\n",
       "1  WILLIAM ROTHMEYER JR                 NaN\n",
       "2       WILLIAM ASBY JR                 NaN\n",
       "3     WILLIAM SALTER JR  WILLIAM SHAFFER JR\n",
       "4     ROY MICHAELSON JR   ROY MICHAELSON JR"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=pd.read_csv(\"using-fuzzy.csv\")\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "   To reduce comparison between pairs of observation, grouped on three basis -\n",
    "   - First they are sorted with respect to dob for smoothly grouping.\n",
    "   - For duplicate observation, dob and gender must be same.\n",
    "   - First name is exactly same.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping\n",
    "def grouping(df1):\n",
    "    index_ofgroup_list=[]\n",
    "    st=0\n",
    "    end=0\n",
    "    total=df1.count(axis=0)[0]\n",
    "    for i in range(1,total):\n",
    "        end+=1\n",
    "        name1=df1.loc[st,[\"fn\"]][0]\n",
    "        name2=df1.loc[i,[\"fn\"]][0]\n",
    "        if(name1.find(\" \")!=-1):\n",
    "            name1=name1[0:name1.find(\" \")]\n",
    "        if(name2.find(\" \")!=-1):\n",
    "            name2=name2[0:name2.find(\" \")]\n",
    "\n",
    "        if(name1!=name2):\n",
    "            index_ofgroup_list.append([st,end])\n",
    "            st=end\n",
    "            if(end==total-1):\n",
    "                index_ofgroup_list.append([st,end+1])\n",
    "            #print(st,end)\n",
    "        elif(df1.loc[i,[\"day\"]][0]!=df1.loc[st,[\"day\"]][0] or df1.loc[i,[\"month\"]][0]!=df1.loc[st,[\"month\"]][0] or\n",
    "            df1.loc[i,[\"year\"]][0]!=df1.loc[st,[\"year\"]][0] or df1.loc[i,[\"gn\"]][0]!=df1.loc[st,[\"gn\"]][0]):\n",
    "            index_ofgroup_list.append([st,end])\n",
    "            st=end\n",
    "            if(end==total-1):\n",
    "                index_ofgroup_list.append([st,end+1])\n",
    "    return index_ofgroup_list\n",
    "                #print(st,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups with respect to same gender and DOB\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "index_ofgroup_list=grouping(df1)\n",
    "print('Total groups with respect to same gender and DOB')\n",
    "print(len(index_ofgroup_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1 by fuzzy matching in strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique IDs obtained by Fuzzy Matching\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "def approach1(df1):\n",
    "    df1[\"stay\"]=True\n",
    "    for index in index_ofgroup_list:\n",
    "        for i in range(index[0],index[1]):\n",
    "            for j in range(i+1,index[1]):\n",
    "                fn1=df1.loc[i,[\"fullname\"]][0]\n",
    "                fn2=df1.loc[j,[\"fullname\"]][0]\n",
    "                if(fuzz.token_set_ratio(fn1,fn2)>90):\n",
    "                    df1.set_value(j,\"stay\",False)\n",
    "    \n",
    "    df1=df1[df1[\"stay\"]==True]\n",
    "    df1=df1.reset_index(drop=True)\n",
    "    df1=df1.drop([\"stay\"],axis=1)\n",
    "    return df1\n",
    "df2=approach1(df1)\n",
    "dfa=df2.copy()\n",
    "dfa['dob']=dfa['day'].astype(str)+'/'+dfa['month'].astype(str)+'/'+dfa['year'].astype(str)\n",
    "dfa=dfa.drop(['day','month','year','fullname'],axis=1)\n",
    "dfa.to_csv(os.getcwd()+'/Fuzzy.csv')\n",
    "print('No. of unique IDs obtained by Fuzzy Matching')\n",
    "print(df2.count()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2 - Training with pairs of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare(f_1, f_2):\n",
    "    if f_1 and f_2 :\n",
    "        f_1=str(f_1).split('/')\n",
    "        f_2=str(f_2).split('/')\n",
    "\n",
    "        if f_1[0] == f_2[0] and f_1[1] == f_2[1] and f_1[2] == f_2[2] :\n",
    "            return 0\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'csv_example_training.json'\n",
    "variables = [{'field' : 'ln', 'type': 'String'},\n",
    "               {'field' : 'dob', 'type': 'Custom', 'comparator' : Compare },\n",
    "#              {'field' : 'day','type' : \"Exact\"},\n",
    "#              {'field' : 'month','type' : \"Exact\"},\n",
    "#              {'field' : 'year','type' : \"Exact\"},\n",
    "             {'field' : 'gn', 'type': 'Categorical','categories':[\"M\",\"F\"]},\n",
    "            {'field' : 'fn', 'type': 'String'},\n",
    "\n",
    "             ]\n",
    "Deduper = dedupe.Dedupe(variables)\n",
    "dt=df.T.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling on the basis of results obtained from fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "match=[]\n",
    "distinct=[]\n",
    "key=0\n",
    "\n",
    "for i in range(len(dt)):\n",
    "    for j in range(len(dt)):\n",
    "        if(i!=j):\n",
    "            fn1=dt[i][\"fn\"]+dt[i][\"ln\"]\n",
    "            fn2=dt[j][\"fn\"]+dt[j][\"ln\"]\n",
    "            if(fuzz.token_set_ratio(fn1,fn2)>90 and dt[i]['dob']==dt[j]['dob'] and\n",
    "                dt[i]['gn']==dt[j]['gn']):\n",
    "                match.append((dt[i],dt[j]))\n",
    "            elif(75<fuzz.token_set_ratio(fn1,fn2) and fuzz.token_set_ratio(fn1,fn2)<90 or \n",
    "                dt[i]['dob']!=dt[j]['dob'] or dt[i]['gn']!=dt[j]['gn']):\n",
    "                distinct.append((dt[i],dt[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 9810)\n"
     ]
    }
   ],
   "source": [
    "print(len(match),len(distinct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct=distinct[0:3*len(match)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_eg = {'match':match,'distinct':distinct}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Classifiers \n",
    "   They can be easily modified by replacing the Deduper classifier with required accordingly.\n",
    "   Note - The replaced classifier must have two methods fit and predict proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# Deduper.classifier=GaussianProcessClassifier()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Deduper.classifier=RandomForestClassifier(max_depth=6,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deduper.sample(dt,15000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(LevenshteinCanopyPredicate: (2, fn), SimplePredicate: (sameFiveCharStartPredicate, ln))\n"
     ]
    }
   ],
   "source": [
    "Deduper.markPairs(labeled_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(LevenshteinCanopyPredicate: (2, fn), TfidfNGramCanopyPredicate: (0.8, ln))\n"
     ]
    }
   ],
   "source": [
    "Deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reading labeled examples from ', 'csv_example_training.json')\n"
     ]
    }
   ],
   "source": [
    "print('reading labeled examples from ', training_file)\n",
    "with open(os.getcwd()+'/'+training_file, 'w') as f:\n",
    "    Deduper.writeTraining(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:Maximum expected recall and precision\n",
      "INFO:dedupe.api:recall: 0.988\n",
      "INFO:dedupe.api:precision: 0.995\n",
      "INFO:dedupe.api:With threshold: 0.700\n"
     ]
    }
   ],
   "source": [
    "threshold = Deduper.threshold(dt, recall_weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate sets and Cluster ID's with predicted probability for the pair observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n",
      "('# duplicate sets', 27)\n"
     ]
    }
   ],
   "source": [
    "print('clustering...')\n",
    "clustered_dupes = Deduper.match(dt, threshold)\n",
    "\n",
    "print('# duplicate sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.getcwd()+'/'+'input.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=os.getcwd()+'/'+'input.csv'\n",
    "output_file=os.getcwd()+'/'+'output.csv'\n",
    "final_file=os.getcwd()+'/csv_final_output.csv'\n",
    "cluster_membership = {}\n",
    "cluster_id = 0\n",
    "for (cluster_id, cluster) in enumerate(clustered_dupes):\n",
    "    id_set, scores = cluster\n",
    "    cluster_d = [dt[c] for c in id_set]\n",
    "    canonical_rep = dedupe.canonicalize(cluster_d)\n",
    "    for record_id, score in zip(id_set, scores):\n",
    "        cluster_membership[record_id] = {\n",
    "            \"cluster id\" : cluster_id,\n",
    "            \"canonical representation\" : canonical_rep,\n",
    "            \"confidence\": score\n",
    "        }\n",
    "\n",
    "singleton_id = cluster_id + 1\n",
    "\n",
    "with open(output_file, 'w') as f_output, open(input_file,'rU') as f_input:\n",
    "    writer = csv.writer(f_output)\n",
    "    reader = csv.reader(f_input)\n",
    "    \n",
    "    heading_row = next(reader)\n",
    "    heading_row.insert(0, 'confidence_score')\n",
    "    heading_row.insert(0, 'Cluster ID')\n",
    "    canonical_keys = canonical_rep.keys()\n",
    "    for key in canonical_keys:\n",
    "        heading_row.append('canonical_' + key)\n",
    "\n",
    "    writer.writerow(heading_row)\n",
    "\n",
    "    for row in reader:\n",
    "        row_id = int(row[0])\n",
    "        if row_id in cluster_membership:\n",
    "            cluster_id = cluster_membership[row_id][\"cluster id\"]\n",
    "            canonical_rep = cluster_membership[row_id][\"canonical representation\"]\n",
    "            row.insert(0, cluster_membership[row_id]['confidence'])\n",
    "            row.insert(0, cluster_id)\n",
    "            for key in canonical_keys:\n",
    "                row.append(canonical_rep[key].encode('utf8'))\n",
    "        else:\n",
    "            row.insert(0, None)\n",
    "            row.insert(0, singleton_id)\n",
    "            singleton_id += 1\n",
    "            for key in canonical_keys:\n",
    "                row.append(None)\n",
    "        writer.writerow(row)\n",
    "\n",
    "f_output.close()\n",
    "with open(output_file, 'r') as f_output, open(final_file,'w') as fin_output:\n",
    "    reader = csv.reader(f_output)\n",
    "    writer = csv.writer(fin_output)\n",
    "    head_row=[\"ID\",\"ln\",\"dob\",\"gn\",\"fn\"]\n",
    "    writer.writerow(head_row)\n",
    "    uniClusterId=list()\n",
    "    i=1\n",
    "    for row in reader:\n",
    "        if i==1:\n",
    "            i+=1\n",
    "            continue\n",
    "        if row[0] in uniClusterId:\n",
    "            continue\n",
    "        else:\n",
    "            uniClusterId.append(row[0])\n",
    "            mid_row=(row[2],row[3],row[4],row[5],row[6])\n",
    "            writer.writerow(mid_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
